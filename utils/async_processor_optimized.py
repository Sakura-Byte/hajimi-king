"""
é‡æ„çš„å¼‚æ­¥å¤„ç†å™¨ - é›†æˆæ‰€æœ‰å¹¶å‘ä¼˜åŒ–
åŸºäºAPI rate limitçš„æ™ºèƒ½å¹¶å‘æ¨¡å‹
"""

import asyncio
import random
import time
from dataclasses import dataclass
from datetime import datetime
from typing import Any

import aiohttp
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions

from common.config import Config
from common.Logger import logger
from utils.file_manager import file_manager
from utils.sync_utils import sync_utils

from .rate_limit.adaptive_scheduler import AdaptiveScheduler, TaskType
from .rate_limit.backpressure_rl import RateLimitBackpressure
from .rate_limit.priority_queue import FileTaskInfo, KeyValidationTaskInfo, PriorityCalculator, QuotaAwarePriorityQueue

# å¯¼å…¥æ–°çš„å¹¶å‘ä¼˜åŒ–ç»„ä»¶
from .rate_limit.quota_monitor import QuotaMonitor
from .rate_limit.smart_load_balancer import PredictiveLoadBalancer
from .rate_limit.token_manager import TokenManager
from .rate_limit.work_stealing import WorkStealingScheduler


@dataclass
class OptimizedProcessingStats:
    """ä¼˜åŒ–åçš„å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""

    github_searches: int = 0
    files_downloaded: int = 0
    keys_extracted: int = 0
    keys_validated: int = 0
    valid_keys: int = 0
    rate_limited_keys: int = 0
    errors: int = 0
    start_time: datetime | None = None

    # æ–°å¢ä¼˜åŒ–æŒ‡æ ‡
    avg_queue_wait_time: float = 0.0
    worker_utilization: float = 0.0
    api_quota_efficiency: float = 0.0
    backpressure_activations: int = 0
    work_steals_successful: int = 0
    load_rebalances: int = 0

    def reset(self):
        """é‡ç½®æ‰€æœ‰ç»Ÿè®¡"""
        for field in self.__dataclass_fields__:
            if field != "start_time":
                setattr(self, field, 0 if not field.startswith("avg_") else 0.0)
        self.start_time = datetime.now()


class OptimizedAsyncProcessor:
    """ä¼˜åŒ–çš„å¼‚æ­¥ä»»åŠ¡å¤„ç†å™¨"""

    def __init__(
        self,
        max_file_workers: int = 8,
        max_validation_workers: int = 5,
        file_queue_size: int = 200,
        key_queue_size: int = 100,
        enable_work_stealing: bool = True,
        enable_smart_load_balancing: bool = True,
    ):
        # åŸºç¡€é…ç½®
        self.max_file_workers = max_file_workers
        self.max_validation_workers = max_validation_workers
        self.enable_work_stealing = enable_work_stealing
        self.enable_smart_load_balancing = enable_smart_load_balancing

        # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
        self.quota_monitor = QuotaMonitor()
        self.token_manager = TokenManager(self.quota_monitor)
        self.backpressure = RateLimitBackpressure(self.quota_monitor, self.token_manager)
        self.adaptive_scheduler = AdaptiveScheduler(self.quota_monitor, self.token_manager, self.backpressure)

        # ä¼˜å…ˆçº§é˜Ÿåˆ—
        self.file_priority_queue = QuotaAwarePriorityQueue("github", file_queue_size)
        self.key_priority_queue = QuotaAwarePriorityQueue("gemini", key_queue_size)

        # å·¥ä½œçªƒå–è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
        if enable_work_stealing:
            self.work_stealing_scheduler = WorkStealingScheduler(self.quota_monitor, self.token_manager)
        else:
            self.work_stealing_scheduler = None

        # æ™ºèƒ½è´Ÿè½½å‡è¡¡å™¨ï¼ˆå¯é€‰ï¼‰
        if enable_smart_load_balancing:
            self.load_balancer = PredictiveLoadBalancer(self.quota_monitor, self.token_manager)
        else:
            self.load_balancer = None

        # Workerç®¡ç†
        self.file_workers: list[asyncio.Task] = []
        self.validation_workers: list[asyncio.Task] = []
        self.active_workers = {"file": 0, "validation": 0}

        # æ§åˆ¶å’ŒçŠ¶æ€
        self.shutdown_event: asyncio.Event | None = None
        self.is_running = False
        self.stats = OptimizedProcessingStats()

        # åå°ä»»åŠ¡
        self.monitoring_task: asyncio.Task | None = None
        self.rebalancing_task: asyncio.Task | None = None

        logger.info("ğŸš€ OptimizedAsyncProcessor initialized")
        logger.info(f"   ğŸ“Š Max workers: {max_file_workers} file, {max_validation_workers} validation")
        logger.info(f"   ğŸ¯ Work stealing: {enable_work_stealing}")
        logger.info(f"   âš–ï¸ Smart load balancing: {enable_smart_load_balancing}")

    async def start(self):
        """å¯åŠ¨ä¼˜åŒ–çš„å¼‚æ­¥å¤„ç†å™¨"""
        if self.is_running:
            logger.warning("OptimizedAsyncProcessor is already running")
            return

        logger.info("ğŸš€ Starting OptimizedAsyncProcessor...")

        # åˆ›å»ºshutdownäº‹ä»¶
        self.shutdown_event = asyncio.Event()

        # å¯åŠ¨åˆå§‹worker
        await self._start_initial_workers()

        # å¯åŠ¨åå°ç›‘æ§ä»»åŠ¡
        self.monitoring_task = asyncio.create_task(self._monitoring_loop())

        # å¯åŠ¨è´Ÿè½½é‡æ–°å‡è¡¡ä»»åŠ¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.load_balancer:
            self.rebalancing_task = asyncio.create_task(self._rebalancing_loop())

        self.is_running = True
        self.stats.reset()

        logger.info("âœ… OptimizedAsyncProcessor started")

    async def stop(self):
        """åœæ­¢å¼‚æ­¥å¤„ç†å™¨"""
        if not self.is_running:
            return

        logger.info("ğŸ›‘ Stopping OptimizedAsyncProcessor...")

        # è®¾ç½®shutdownäº‹ä»¶
        if self.shutdown_event:
            self.shutdown_event.set()

        # åœæ­¢åå°ä»»åŠ¡
        if self.monitoring_task:
            self.monitoring_task.cancel()
            try:
                await self.monitoring_task
            except asyncio.CancelledError:
                pass

        if self.rebalancing_task:
            self.rebalancing_task.cancel()
            try:
                await self.rebalancing_task
            except asyncio.CancelledError:
                pass

        # ç­‰å¾…æ‰€æœ‰workerå®Œæˆ
        all_workers = self.file_workers + self.validation_workers
        if all_workers:
            await asyncio.gather(*all_workers, return_exceptions=True)

        self.is_running = False
        logger.info("âœ… OptimizedAsyncProcessor stopped")

    async def add_file_task(self, item: dict[str, Any], query: str, priority: int | None = None) -> bool:
        """æ·»åŠ æ–‡ä»¶å¤„ç†ä»»åŠ¡"""
        if not self.is_running:
            return False

        # æ„å»ºä»»åŠ¡ä¿¡æ¯
        task_info = FileTaskInfo(
            repo_name=item["repository"]["full_name"],
            file_path=item["path"],
            file_size=item.get("size", 1024),  # é»˜è®¤1KB
            repo_stars=item["repository"].get("stargazers_count", 0),
            repo_updated_at=item["repository"].get("pushed_at", ""),
            file_url=item["html_url"],
        )

        # è®¡ç®—ä¼˜å…ˆçº§ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if priority is None:
            quota_status = self.quota_monitor.get_quota_summary()
            calculator = PriorityCalculator()
            priority = calculator.calculate_file_task_priority(task_info, quota_status)

        # æ·»åŠ åˆ°ä¼˜å…ˆçº§é˜Ÿåˆ—
        success = self.file_priority_queue.add_file_task(
            task_id=f"file_{item.get('sha', 'unknown')}",
            task_info=task_info,
            current_quota_status=self.quota_monitor.get_quota_summary(),
            task_data={"item": item, "query": query},
        )

        if success:
            logger.debug(f"ğŸ“¥ Added file task for {task_info.file_path} with priority {priority}")

        return success

    async def _start_initial_workers(self):
        """å¯åŠ¨åˆå§‹worker"""
        # å¯åŠ¨æ–‡ä»¶å¤„ç†worker
        initial_file_workers = min(self.max_file_workers, len(self.quota_monitor.github_tokens) * 2)
        for i in range(initial_file_workers):
            await self._start_file_worker(f"file-worker-{i}")

        # å¯åŠ¨éªŒè¯worker
        for i in range(self.max_validation_workers):
            await self._start_validation_worker(f"validation-worker-{i}")

    async def _start_file_worker(self, worker_id: str):
        """å¯åŠ¨æ–‡ä»¶å¤„ç†worker"""
        if self.work_stealing_scheduler:
            # ä½¿ç”¨å·¥ä½œçªƒå–è°ƒåº¦å™¨
            self.work_stealing_scheduler.register_worker(worker_id, TaskType.GITHUB_FILE)

        worker_task = asyncio.create_task(self._file_worker_with_optimizations(worker_id))
        self.file_workers.append(worker_task)
        self.active_workers["file"] += 1

        logger.debug(f"ğŸ‘· Started file worker: {worker_id}")

    async def _start_validation_worker(self, worker_id: str):
        """å¯åŠ¨éªŒè¯worker"""
        if self.work_stealing_scheduler:
            self.work_stealing_scheduler.register_worker(worker_id, TaskType.GEMINI_VALIDATION)

        worker_task = asyncio.create_task(self._validation_worker_with_optimizations(worker_id))
        self.validation_workers.append(worker_task)
        self.active_workers["validation"] += 1

        logger.debug(f"ğŸ” Started validation worker: {worker_id}")

    async def _file_worker_with_optimizations(self, worker_id: str):
        """ä¼˜åŒ–çš„æ–‡ä»¶å¤„ç†worker"""
        logger.info(f"ğŸ‘· {worker_id} started with optimizations")

        while not self.shutdown_event.is_set():
            try:
                # æ£€æŸ¥èƒŒå‹çŠ¶æ€
                if self.backpressure.should_pause_github_tasks():
                    await asyncio.sleep(5)
                    continue

                # è·å–ä»»åŠ¡
                task_data = None
                if self.work_stealing_scheduler:
                    # ä½¿ç”¨å·¥ä½œçªƒå–è°ƒåº¦å™¨
                    scheduled_task = self.work_stealing_scheduler.get_next_task_for_worker(worker_id)
                    if scheduled_task:
                        task_data = scheduled_task.payload
                else:
                    # ä½¿ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—
                    task_result = self.file_priority_queue.get_next_task()
                    if task_result:
                        task_id, task_data = task_result

                if not task_data:
                    await asyncio.sleep(1)
                    continue

                # åº”ç”¨èƒŒå‹å»¶è¿Ÿ
                delay = await self.backpressure.get_delay_for_github_task()
                if delay > 0:
                    await asyncio.sleep(delay)

                # å¤„ç†ä»»åŠ¡
                start_time = time.time()
                await self._process_file_task_optimized(task_data, worker_id)
                time.time() - start_time

                # è®°å½•å®Œæˆ
                if self.work_stealing_scheduler:
                    self.work_stealing_scheduler.mark_task_completed(worker_id, scheduled_task)

                self.stats.files_downloaded += 1

            except Exception as e:
                logger.error(f"âŒ {worker_id} error: {e}")
                self.stats.errors += 1
                await asyncio.sleep(2)  # é”™è¯¯åçŸ­æš‚ä¼‘æ¯

        logger.info(f"ğŸ‘· {worker_id} stopped")

    async def _validation_worker_with_optimizations(self, worker_id: str):
        """ä¼˜åŒ–çš„éªŒè¯worker"""
        logger.info(f"ğŸ” {worker_id} started with optimizations")

        while not self.shutdown_event.is_set():
            try:
                # æ£€æŸ¥èƒŒå‹çŠ¶æ€
                if self.backpressure.should_pause_gemini_tasks():
                    await asyncio.sleep(5)
                    continue

                # è·å–ä»»åŠ¡
                task_data = None
                if self.work_stealing_scheduler:
                    scheduled_task = self.work_stealing_scheduler.get_next_task_for_worker(worker_id)
                    if scheduled_task:
                        task_data = scheduled_task.payload
                else:
                    task_result = self.key_priority_queue.get_next_task()
                    if task_result:
                        task_id, task_data = task_result

                if not task_data:
                    await asyncio.sleep(1)
                    continue

                # åº”ç”¨èƒŒå‹å»¶è¿Ÿ
                delay = await self.backpressure.get_delay_for_gemini_task()
                if delay > 0:
                    await asyncio.sleep(delay)

                # å¤„ç†ä»»åŠ¡
                start_time = time.time()
                await self._process_validation_task_optimized(task_data, worker_id)
                time.time() - start_time

                # è®°å½•å®Œæˆ
                if self.work_stealing_scheduler:
                    self.work_stealing_scheduler.mark_task_completed(worker_id, scheduled_task)

                self.stats.keys_validated += 1

            except Exception as e:
                logger.error(f"âŒ {worker_id} error: {e}")
                self.stats.errors += 1
                await asyncio.sleep(2)

        logger.info(f"ğŸ” {worker_id} stopped")

    async def _process_file_task_optimized(self, task_data: dict, worker_id: str):
        """ä¼˜åŒ–çš„æ–‡ä»¶ä»»åŠ¡å¤„ç†"""
        item = task_data["item"]
        task_data["query"]

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        if self._should_skip_item(item):
            return

        # è·å–æœ€ä½³tokenï¼ˆä½¿ç”¨è´Ÿè½½å‡è¡¡å™¨ï¼‰
        token_decision = None
        if self.load_balancer:
            token_decision = await self.load_balancer.get_best_github_token(task_priority=5)

        if not token_decision:
            # Fallbackåˆ°token manager
            token = self.token_manager.get_best_github_token(task_priority=5)
        else:
            token = token_decision.resource_id

        if not token:
            logger.warning(f"ğŸš« {worker_id} no available token")
            return

        # ä¸‹è½½æ–‡ä»¶å†…å®¹
        start_time = time.time()
        content = await self._download_file_content_optimized(item, token, worker_id)
        response_time = time.time() - start_time

        # è®°å½•tokenæ€§èƒ½
        success = content is not None
        self.token_manager.record_token_usage(token, success, response_time)

        if self.load_balancer and token_decision:
            self.load_balancer.record_resource_performance(token, success, response_time)

        if not content:
            return

        # æå–keys
        keys = self._extract_keys_from_content(content)
        if not keys:
            return

        self.stats.keys_extracted += len(keys)

        # ä¸ºæ¯ä¸ªkeyåˆ›å»ºéªŒè¯ä»»åŠ¡
        repo_name = item["repository"]["full_name"]
        file_path = item["path"]
        repo_stars = item["repository"].get("stargazers_count", 0)

        for key in keys:
            # åˆ›å»ºéªŒè¯ä»»åŠ¡ä¿¡æ¯
            task_info = KeyValidationTaskInfo(
                key=key,
                repo_name=repo_name,
                file_path=file_path,
                repo_stars=repo_stars,
                key_source_context=self._get_key_context(content, key),
            )

            # æ·»åŠ åˆ°éªŒè¯é˜Ÿåˆ—
            self.key_priority_queue.add_validation_task(
                task_id=f"key_{hash(key)}",
                task_info=task_info,
                current_quota_status=self.quota_monitor.get_quota_summary(),
                task_data={"key": key, "repo_name": repo_name, "file_path": file_path, "file_url": item["html_url"]},
            )

    async def _process_validation_task_optimized(self, task_data: dict, worker_id: str):
        """ä¼˜åŒ–çš„éªŒè¯ä»»åŠ¡å¤„ç†"""
        key = task_data["key"]
        repo_name = task_data["repo_name"]
        file_path = task_data["file_path"]
        file_url = task_data["file_url"]

        # éªŒè¯key
        start_time = time.time()
        result = await self._validate_gemini_key_async(key, worker_id)
        response_time = time.time() - start_time

        # è®°å½•Geminiæ€§èƒ½
        success = result and "ok" in result
        is_429 = result == "rate_limited" or "429" in str(result)
        self.quota_monitor.record_gemini_call(success, response_time, is_429)

        # å¤„ç†éªŒè¯ç»“æœ
        if success:
            self.stats.valid_keys += 1
            logger.info(f"âœ… {worker_id} VALID: {key}")

            file_manager.save_valid_keys(repo_name, file_path, file_url, [key])

            try:
                sync_utils.add_keys_to_queue([key])
            except Exception as e:
                logger.error(f"ğŸ“¥ {worker_id} sync queue error: {e}")

        elif result == "rate_limited":
            self.stats.rate_limited_keys += 1
            logger.warning(f"âš ï¸ {worker_id} RATE LIMITED: {key}")
            file_manager.save_rate_limited_keys(repo_name, file_path, file_url, [key])

    async def _download_file_content_optimized(self, item: dict[str, Any], token: str, worker_id: str) -> str | None:
        """ä¼˜åŒ–çš„æ–‡ä»¶å†…å®¹ä¸‹è½½"""
        repo_full_name = item["repository"]["full_name"]
        file_path = item["path"]
        metadata_url = f"https://api.github.com/repos/{repo_full_name}/contents/{file_path}"

        headers = {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Authorization": f"token {token}",
        }

        # è·å–æœ€ä½³ä»£ç†
        proxy_config = None
        if self.load_balancer:
            proxy_decision = await self.load_balancer.get_best_proxy()
            if proxy_decision:
                proxy_url = Config.PROXY_LIST[0] if Config.PROXY_LIST else None  # ç®€åŒ–å®ç°
                if proxy_url:
                    proxy_config = {"http": proxy_url, "https": proxy_url}

        if not proxy_config:
            proxy_config = self.token_manager.get_best_proxy()

        try:
            timeout = aiohttp.ClientTimeout(total=30)
            connector = aiohttp.TCPConnector()

            async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
                async with session.get(
                    metadata_url, headers=headers, proxy=proxy_config.get("http") if proxy_config else None
                ) as response:
                    # æ›´æ–°é…é¢çŠ¶æ€
                    await self.quota_monitor.update_github_quota(token, dict(response.headers))

                    response.raise_for_status()
                    file_metadata = await response.json()

                    # å°è¯•base64è§£ç 
                    encoding = file_metadata.get("encoding")
                    content = file_metadata.get("content")

                    if encoding == "base64" and content:
                        import base64

                        try:
                            decoded_content = base64.b64decode(content).decode("utf-8")
                            return decoded_content
                        except Exception:
                            pass

                    # ä½¿ç”¨download_url
                    download_url = file_metadata.get("download_url")
                    if not download_url:
                        return None

                    async with session.get(
                        download_url, headers=headers, proxy=proxy_config.get("http") if proxy_config else None
                    ) as content_response:
                        content_response.raise_for_status()
                        return await content_response.text()

        except Exception as e:
            logger.error(f"âŒ {worker_id} download failed: {metadata_url} - {type(e).__name__}")
            return None

    async def _validate_gemini_key_async(self, api_key: str, worker_id: str) -> str:
        """å¼‚æ­¥éªŒè¯Gemini API key"""
        try:
            # éšæœºå»¶è¿Ÿé¿å…rate limit
            await asyncio.sleep(random.uniform(0.5, 1.5))

            # åœ¨æ–°çš„çº¿ç¨‹ä¸­æ‰§è¡ŒåŒæ­¥çš„Gemini APIè°ƒç”¨
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, self._validate_gemini_key_sync, api_key)
            return result

        except Exception as e:
            logger.error(f"âŒ {worker_id} validation error: {e}")
            return f"error:{e.__class__.__name__}"

    def _validate_gemini_key_sync(self, api_key: str) -> str:
        """åŒæ­¥éªŒè¯Gemini API key (åœ¨executorä¸­è¿è¡Œ)"""
        try:
            # é…ç½®Geminiå®¢æˆ·ç«¯
            genai.configure(api_key=api_key, client_options={"api_endpoint": "generativelanguage.googleapis.com"})

            # å‘é€æµ‹è¯•è¯·æ±‚
            model = genai.GenerativeModel(Config.HAJIMI_CHECK_MODEL)
            model.generate_content("hi")
            return "ok"

        except (google_exceptions.PermissionDenied, google_exceptions.Unauthenticated):
            return "not_authorized_key"
        except google_exceptions.TooManyRequests:
            return "rate_limited"
        except Exception as e:
            if "429" in str(e) or "rate limit" in str(e).lower() or "quota" in str(e).lower():
                return "rate_limited:429"
            if "403" in str(e) or "SERVICE_DISABLED" in str(e) or "API has not been used" in str(e):
                return "disabled"
            return f"error:{e.__class__.__name__}"

    async def _monitoring_loop(self):
        """åå°ç›‘æ§å¾ªç¯"""
        logger.info("ğŸ“Š Starting monitoring loop")

        while not self.shutdown_event.is_set():
            try:
                await asyncio.sleep(30)  # æ¯30ç§’ç›‘æ§ä¸€æ¬¡

                # æ›´æ–°workeræ•°é‡é™åˆ¶
                await self.adaptive_scheduler.update_worker_limits()

                # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´workeræ•°é‡
                await self._adjust_worker_count()

                # æ¸…ç†è¿‡æœŸä»»åŠ¡
                self.file_priority_queue.clear_low_priority_tasks()
                self.key_priority_queue.clear_low_priority_tasks()

                # é‡æ–°è¯„ä¼°ä»»åŠ¡ä¼˜å…ˆçº§
                if random.random() < 0.1:  # 10%æ¦‚ç‡é‡æ–°è¯„ä¼°
                    current_quota = self.quota_monitor.get_quota_summary()
                    self.file_priority_queue.reprioritize_tasks(current_quota)
                    self.key_priority_queue.reprioritize_tasks(current_quota)

            except Exception as e:
                logger.error(f"âŒ Monitoring loop error: {e}")
                await asyncio.sleep(10)

    async def _rebalancing_loop(self):
        """è´Ÿè½½é‡æ–°å‡è¡¡å¾ªç¯"""
        if not self.load_balancer:
            return

        logger.info("âš–ï¸ Starting load rebalancing loop")

        while not self.shutdown_event.is_set():
            try:
                await asyncio.sleep(120)  # æ¯2åˆ†é’Ÿé‡æ–°å‡è¡¡ä¸€æ¬¡

                rebalance_result = self.load_balancer.rebalance_load()
                if rebalance_result["actions_taken"] > 0:
                    self.stats.load_rebalances += 1
                    logger.info(f"âš–ï¸ Load rebalanced with {rebalance_result['actions_taken']} actions")

            except Exception as e:
                logger.error(f"âŒ Rebalancing loop error: {e}")
                await asyncio.sleep(30)

    async def _adjust_worker_count(self):
        """åŠ¨æ€è°ƒæ•´workeræ•°é‡"""
        # è·å–æ¨èçš„workeræ•°é‡
        recommended_github = self.quota_monitor.get_recommended_github_workers()
        recommended_gemini = self.quota_monitor.get_recommended_gemini_workers()

        # è°ƒæ•´æ–‡ä»¶workeræ•°é‡
        current_file_workers = len(self.file_workers)
        if recommended_github > current_file_workers and recommended_github <= self.max_file_workers:
            # å¢åŠ worker
            for i in range(recommended_github - current_file_workers):
                worker_id = f"file-worker-dynamic-{int(time.time())}-{i}"
                await self._start_file_worker(worker_id)
                logger.info(f"ğŸ“ˆ Added file worker: {worker_id}")

        elif recommended_github < current_file_workers:
            # å‡å°‘worker (é€šè¿‡ä¸å¯åŠ¨æ–°ä»»åŠ¡è‡ªç„¶å‡å°‘)
            excess_workers = current_file_workers - recommended_github
            logger.info(f"ğŸ“‰ Will naturally reduce {excess_workers} file workers")

        # è°ƒæ•´éªŒè¯workeræ•°é‡
        current_validation_workers = len(self.validation_workers)
        if recommended_gemini > current_validation_workers and recommended_gemini <= self.max_validation_workers:
            for i in range(recommended_gemini - current_validation_workers):
                worker_id = f"validation-worker-dynamic-{int(time.time())}-{i}"
                await self._start_validation_worker(worker_id)
                logger.info(f"ğŸ“ˆ Added validation worker: {worker_id}")

    def _should_skip_item(self, item: dict[str, Any]) -> tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡å¤„ç†æ­¤item"""
        # å¤ç”¨åŸæ¥çš„é€»è¾‘
        from utils.async_processor import AsyncProcessor

        temp_processor = AsyncProcessor()
        return temp_processor._should_skip_item(item)

    def _extract_keys_from_content(self, content: str) -> list[str]:
        """ä»æ–‡ä»¶å†…å®¹ä¸­æå–API keys"""
        # å¤ç”¨åŸæ¥çš„é€»è¾‘
        from utils.async_processor import AsyncProcessor

        temp_processor = AsyncProcessor()
        return temp_processor._extract_keys_from_content(content)

    def _get_key_context(self, content: str, key: str) -> str:
        """è·å–keyå‘¨å›´çš„ä¸Šä¸‹æ–‡"""
        key_index = content.find(key)
        if key_index == -1:
            return ""

        start = max(0, key_index - 50)
        end = min(len(content), key_index + len(key) + 50)
        return content[start:end]

    def get_optimized_stats(self) -> dict:
        """è·å–ä¼˜åŒ–åçš„ç»Ÿè®¡ä¿¡æ¯"""
        base_stats = {
            "github_searches": self.stats.github_searches,
            "files_downloaded": self.stats.files_downloaded,
            "keys_extracted": self.stats.keys_extracted,
            "keys_validated": self.stats.keys_validated,
            "valid_keys": self.stats.valid_keys,
            "rate_limited_keys": self.stats.rate_limited_keys,
            "errors": self.stats.errors,
        }

        # æ·»åŠ ä¼˜åŒ–æŒ‡æ ‡
        quota_summary = self.quota_monitor.get_quota_summary()
        backpressure_summary = self.backpressure.get_status_summary()

        optimized_stats = {
            **base_stats,
            "optimization_metrics": {
                "backpressure_level": backpressure_summary["level"],
                "github_health_score": quota_summary["github"]["avg_health_score"],
                "gemini_health": "good" if not quota_summary["gemini"]["recent_429"] else "degraded",
                "queue_sizes": {
                    "file_queue": self.file_priority_queue.get_queue_stats()["current_size"],
                    "key_queue": self.key_priority_queue.get_queue_stats()["current_size"],
                },
                "worker_counts": {
                    "file_workers": len(self.file_workers),
                    "validation_workers": len(self.validation_workers),
                },
            },
        }

        # æ·»åŠ å·¥ä½œçªƒå–ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.work_stealing_scheduler:
            stealing_stats = self.work_stealing_scheduler.get_scheduler_stats()
            optimized_stats["work_stealing"] = stealing_stats

        # æ·»åŠ è´Ÿè½½å‡è¡¡ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.load_balancer:
            lb_stats = self.load_balancer.get_load_balancer_stats()
            optimized_stats["load_balancing"] = lb_stats

        return optimized_stats

    def get_queue_status(self) -> dict[str, Any]:
        """è·å–é˜Ÿåˆ—çŠ¶æ€"""
        return {
            "file_queue": self.file_priority_queue.get_queue_stats(),
            "key_queue": self.key_priority_queue.get_queue_stats(),
            "backpressure": self.backpressure.get_status_summary(),
            "quota_summary": self.quota_monitor.get_quota_summary(),
        }
