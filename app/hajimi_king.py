import asyncio
import sys
import traceback
from datetime import datetime

from common.Logger import logger

sys.path.append("../")
from common.config import Config
from utils.async_processor_optimized import OptimizedAsyncProcessor
from utils.file_manager import checkpoint, file_manager
from utils.github_client import GitHubClient
from utils.sync_utils import sync_utils

# åˆ›å»ºGitHubå·¥å…·å®žä¾‹å’Œæ–‡ä»¶ç®¡ç†å™¨
github_utils = GitHubClient.create_instance(Config.GITHUB_TOKENS)


def normalize_query(query: str) -> str:
    query = " ".join(query.split())

    parts = []
    i = 0
    while i < len(query):
        if query[i] == '"':
            end_quote = query.find('"', i + 1)
            if end_quote != -1:
                parts.append(query[i : end_quote + 1])
                i = end_quote + 1
            else:
                parts.append(query[i])
                i += 1
        elif query[i] == " ":
            i += 1
        else:
            start = i
            while i < len(query) and query[i] != " ":
                i += 1
            parts.append(query[start:i])

    quoted_strings = []
    language_parts = []
    filename_parts = []
    path_parts = []
    other_parts = []

    for part in parts:
        if part.startswith('"') and part.endswith('"'):
            quoted_strings.append(part)
        elif part.startswith("language:"):
            language_parts.append(part)
        elif part.startswith("filename:"):
            filename_parts.append(part)
        elif part.startswith("path:"):
            path_parts.append(part)
        elif part.strip():
            other_parts.append(part)

    normalized_parts = []
    normalized_parts.extend(sorted(quoted_strings))
    normalized_parts.extend(sorted(other_parts))
    normalized_parts.extend(sorted(language_parts))
    normalized_parts.extend(sorted(filename_parts))
    normalized_parts.extend(sorted(path_parts))

    return " ".join(normalized_parts)


async def async_main():
    """å¼‚æ­¥ä¸»å‡½æ•° - æ”¯æŒå¹¶å‘å¤„ç†"""
    start_time = datetime.now()

    # æ‰“å°ç³»ç»Ÿå¯åŠ¨ä¿¡æ¯
    logger.info("=" * 60)
    logger.info("ðŸš€ HAJIMI KING STARTING (ASYNC MODE)")
    logger.info("=" * 60)
    logger.info(f"â° Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

    # 1. æ£€æŸ¥é…ç½®
    if not Config.check():
        logger.info("âŒ Config check failed. Exiting...")
        sys.exit(1)
    # 2. æ£€æŸ¥æ–‡ä»¶ç®¡ç†å™¨
    if not file_manager.check():
        logger.error("âŒ FileManager check failed. Exiting...")
        sys.exit(1)

    # 2.5. æ˜¾ç¤ºSyncUtilsçŠ¶æ€å’Œé˜Ÿåˆ—ä¿¡æ¯
    if sync_utils.balancer_enabled:
        logger.info("ðŸ”— SyncUtils ready for async key syncing")

    # æ˜¾ç¤ºé˜Ÿåˆ—çŠ¶æ€
    balancer_queue_count = len(checkpoint.wait_send_balancer)
    gpt_load_queue_count = len(checkpoint.wait_send_gpt_load)
    logger.info(f"ðŸ“Š Queue status - Balancer: {balancer_queue_count}, GPT Load: {gpt_load_queue_count}")

    # 3. æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    search_queries = file_manager.get_search_queries()
    logger.info("ðŸ“‹ SYSTEM INFORMATION:")
    logger.info(f"ðŸ”‘ GitHub tokens: {len(Config.GITHUB_TOKENS)} configured")
    logger.info(f"ðŸ” Search queries: {len(search_queries)} loaded")
    logger.info(f"ðŸ“… Date filter: {Config.DATE_RANGE_DAYS} days")
    if Config.PROXY_LIST:
        logger.info(f"ðŸŒ Proxy: {len(Config.PROXY_LIST)} proxies configured")

    if checkpoint.last_scan_time:
        logger.info("ðŸ’¾ Checkpoint found - Incremental scan mode")
        logger.info(f"   Last scan: {checkpoint.last_scan_time}")
        logger.info(f"   Scanned files: {len(checkpoint.scanned_shas)}")
        logger.info(f"   Processed queries: {len(checkpoint.processed_queries)}")
    else:
        logger.info("ðŸ’¾ No checkpoint - Full scan mode")

    # 4. åˆ›å»ºå¹¶å¯åŠ¨ä¼˜åŒ–çš„å¼‚æ­¥å¤„ç†å™¨
    processor = OptimizedAsyncProcessor(
        max_file_workers=len(Config.GITHUB_TOKENS),  # é™åˆ¶ä¸ºtokenæ•°é‡ï¼Œ1ä¸ªtokenå¯¹åº”1ä¸ªworker
        max_validation_workers=Config.MAX_VALIDATION_WORKERS,
        file_queue_size=Config.FILE_QUEUE_SIZE,
        key_queue_size=Config.KEY_QUEUE_SIZE,
        enable_work_stealing=Config.ENABLE_WORK_STEALING,
        enable_smart_load_balancing=Config.ENABLE_SMART_LOAD_BALANCING,
    )

    await processor.start()

    logger.info("âœ… System ready - Starting concurrent processing")
    logger.info("=" * 60)

    total_queries_processed = 0
    loop_count = 0

    try:
        while True:
            loop_count += 1
            logger.info(f"ðŸ”„ Loop #{loop_count} - {datetime.now().strftime('%H:%M:%S')}")

            query_count = 0
            loop_tasks_added = 0

            for i, q in enumerate(search_queries, 1):
                normalized_q = normalize_query(q)
                if normalized_q in checkpoint.processed_queries:
                    logger.info(f"ðŸ” Skipping already processed query: [{q}], index:#{i}")
                    continue

                # GitHubæœç´¢
                res = github_utils.search_for_keys(q)

                if res and "items" in res:
                    items = res["items"]
                    if items:
                        logger.info(f"ðŸ” Query {i}/{len(search_queries)}: ã€{q}ã€‘ found {len(items)} items")

                        # å°†æ‰€æœ‰itemsæ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—
                        tasks_added = 0
                        for item in items:
                            # è®°å½•SHAåˆ°checkpoint (ç«‹å³è®°å½•é¿å…é‡å¤å¤„ç†)
                            checkpoint.add_scanned_sha(item.get("sha"))

                            # æ·»åŠ åˆ°å¼‚æ­¥å¤„ç†é˜Ÿåˆ—
                            success = await processor.add_file_task(item, q)
                            if success:
                                tasks_added += 1

                        loop_tasks_added += tasks_added
                        logger.info(f"ðŸ“¥ Added {tasks_added}/{len(items)} tasks to processing queue")
                    else:
                        logger.info(f"ðŸ“­ Query {i}/{len(search_queries)} - No items found")

                    # Only mark query as processed when search succeeds
                    checkpoint.add_processed_query(normalized_q)
                    query_count += 1
                    total_queries_processed += 1
                else:
                    logger.warning(f"âŒ Query {i}/{len(search_queries)} failed - will retry in next loop")
                    # Don't mark as processed, don't increment counters

                # å®šæœŸä¿å­˜checkpoint
                if query_count % 5 == 0:
                    checkpoint.update_scan_time()
                    file_manager.save_checkpoint(checkpoint)
                    file_manager.update_dynamic_filenames()

                    # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
                    stats = processor.get_optimized_stats()
                    queue_status = processor.get_queue_status()
                    logger.info(
                        f"ðŸ“Š Progress - Queries: {total_queries_processed}, Files: {stats['files_downloaded']}, Keys found: {stats['keys_extracted']}, Valid: {stats['valid_keys']}, Rate limited: {stats['rate_limited_keys']}"
                    )
                    logger.info(
                        f"ðŸ“¦ Queue status - File queue: {queue_status['file_queue']['current_size']}, Key queue: {queue_status['key_queue']['current_size']}"
                    )

                    # æ˜¾ç¤ºä¼˜åŒ–æŒ‡æ ‡
                    opt_metrics = stats.get("optimization_metrics", {})
                    logger.info(
                        f"ðŸŽ¯ Optimization - Backpressure: {opt_metrics.get('backpressure_level', 'UNKNOWN')}, GitHub health: {opt_metrics.get('github_health_score', 0):.2f}, Workers: F{opt_metrics.get('worker_counts', {}).get('file_workers', 0)}/V{opt_metrics.get('worker_counts', {}).get('validation_workers', 0)}"
                    )

                # çŸ­æš‚ä¼‘æ¯é¿å…APIé™åˆ¶
                if query_count % 3 == 0:
                    logger.info(f"â¸ï¸ Processed {query_count} queries, taking a break...")
                    await asyncio.sleep(2)

            # æ›´æ–°checkpoint
            checkpoint.update_scan_time()
            file_manager.save_checkpoint(checkpoint)
            file_manager.update_dynamic_filenames()

            # æ˜¾ç¤ºå¾ªçŽ¯ç»“æžœ
            stats = processor.get_optimized_stats()
            logger.info(f"ðŸ Loop #{loop_count} complete - Added {loop_tasks_added} tasks | Total stats:")
            logger.info(f"   ðŸ“Š Files processed: {stats['files_downloaded']}, Keys found: {stats['keys_extracted']}")
            logger.info(f"   âœ… Valid keys: {stats['valid_keys']}, âš ï¸ Rate limited: {stats['rate_limited_keys']}")
            logger.info(f"   âŒ Errors: {stats['errors']}")

            # ç­‰å¾…é˜Ÿåˆ—å¤„ç†å®Œæˆ (æœ€å¤šç­‰å¾…30ç§’)
            logger.info("â³ Waiting for queues to process...")
            wait_time = 0
            while wait_time < 30:
                queue_status = processor.get_queue_status()
                file_queue_size = queue_status["file_queue"]["current_size"]
                key_queue_size = queue_status["key_queue"]["current_size"]
                if file_queue_size == 0 and key_queue_size == 0:
                    break
                await asyncio.sleep(1)
                wait_time += 1

                # æ¯5ç§’æ˜¾ç¤ºä¸€æ¬¡é˜Ÿåˆ—çŠ¶æ€
                if wait_time % 5 == 0:
                    logger.info(f"ðŸ“¦ Still processing - File queue: {file_queue_size}, Key queue: {key_queue_size}")

            logger.info("ðŸ’¤ Sleeping for 10 seconds before next loop...")
            await asyncio.sleep(10)

    except KeyboardInterrupt:
        logger.info("â›” Interrupted by user")
    except Exception as e:
        logger.error(f"ðŸ’¥ Unexpected error: {e}")
        traceback.print_exc()
    finally:
        # ä¼˜é›…å…³é—­
        logger.info("ðŸ›‘ Shutting down...")
        await processor.stop()

        checkpoint.update_scan_time()
        file_manager.save_checkpoint(checkpoint)

        # æœ€ç»ˆç»Ÿè®¡
        final_stats = processor.get_optimized_stats()
        logger.info(
            f"ðŸ“Š Final stats - Valid keys: {final_stats['valid_keys']}, Rate limited: {final_stats['rate_limited_keys']}"
        )

        # æ˜¾ç¤ºä¼˜åŒ–æ•ˆæžœæ€»ç»“
        opt_metrics = final_stats.get("optimization_metrics", {})
        logger.info("ðŸŽ¯ Optimization summary:")
        logger.info(f"   Final backpressure level: {opt_metrics.get('backpressure_level', 'UNKNOWN')}")
        logger.info(f"   GitHub health score: {opt_metrics.get('github_health_score', 0):.2f}")
        logger.info(
            f"   Final worker count: {opt_metrics.get('worker_counts', {}).get('file_workers', 0)} file, {opt_metrics.get('worker_counts', {}).get('validation_workers', 0)} validation"
        )

        # æ˜¾ç¤ºé«˜çº§ç»Ÿè®¡ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        if "work_stealing" in final_stats:
            ws_stats = final_stats["work_stealing"]["steal_stats"]
            logger.info(
                f"   Work stealing: {ws_stats['total_steals_successful']}/{ws_stats['total_steals_attempted']} successful"
            )

        if "load_balancing" in final_stats:
            lb_stats = final_stats["load_balancing"]["global_stats"]
            logger.info(
                f"   Load balancing: {lb_stats['failover_events']} failovers, {lb_stats['load_redistributions']} rebalances"
            )
        logger.info("ðŸ”š Shutting down sync utils...")
        sync_utils.shutdown()


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨å¼‚æ­¥å¹¶å‘æ¨¡å¼"""
    asyncio.run(async_main())


if __name__ == "__main__":
    main()
